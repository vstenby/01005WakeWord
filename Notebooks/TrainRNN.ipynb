{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47f2369f-8501-439f-b138-eb6e7dead030",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fc0eac-b3ae-44a1-ac08-3f28e3ab910a",
   "metadata": {},
   "source": [
    "Here, the dataset is generated using:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324eb71b-ad67-4edb-acdc-592143ba9b98",
   "metadata": {},
   "source": [
    "`python downloader.py --export-folder 'RNN_data' --splits 'train' --cliplength 10 --balance 1:1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c435272-c064-42a6-80b9-22a5a5c85e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Import torch stuff.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "#pip install git+https://github.com/facebookresearch/WavAugment.git\n",
    "import augment\n",
    "\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "import bom1.wakeword as wf\n",
    "import bom1.bom1 as bom1\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import os\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a15dd7-bba1-4bcc-98f5-8a547b3677f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the notebook to run on the GPU, if available.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f'This notebook is running on the {device.type}.')\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.current_device()\n",
    "    torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42f4504-8812-4cff-9fb8-ac6e2392c5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS A TOO SIMPLE WAY OF DOING IT!\n",
    "def generate_targets(c, n = 1375):\n",
    "    #c is the class, either 1 or 0.\n",
    "    target = torch.zeros((n))\n",
    "    if c == 0:\n",
    "        #Class 0\n",
    "        return target\n",
    "    else:\n",
    "        #Class 1\n",
    "        target[np.where((np.linspace(0, 10, n) > 6)&(np.linspace(0, 10, n) <= 6.36))[0]] = 1\n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ea3837-2bf2-4cf4-a645-8d238b5035b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WakewordDataset(Dataset):\n",
    "    '''\n",
    "    Construct a dataset with sound files.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, f, folder, sr = 22050, normalize = True, transforms=None):\n",
    "        \n",
    "        self.paths  = [os.path.join(folder, x) for x in os.listdir(folder) if 'C_1' in x] #At the moment, we only want cases where we actually have a \"Kan I se det\" in the sentence.\n",
    "\n",
    "        folderinfo  = [wf.info_from_path(x) for x in self.paths] #Already here, it's shuffled.\n",
    "        self.ID, self.t1, self.t2, self.target = [x[0] for x in folderinfo], [x[1] for x in folderinfo], [x[2] for x in folderinfo], [x[3] for x in folderinfo]\n",
    "         \n",
    "        self.transforms = transforms\n",
    "        self.f          = f\n",
    "        self.normalize  = normalize\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path            = self.paths[idx]\n",
    "        \n",
    "        audio, sr, x    = wf.load_data(path, f = self.f, transforms=self.transforms, normalize=self.normalize)\n",
    "        target          = generate_targets(self.target[idx])\n",
    "        ID              = self.ID[idx] \n",
    "        \n",
    "        return audio, sr, x, target, path, ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950307d9-2521-4b7c-bae4-3be38d28227f",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/work3/s164419/01005WakeWordData/RNN_data/train/'\n",
    "train_dataset = WakewordDataset(f=T.Spectrogram(hop_length=40), folder=folder, normalize=True, #normalize the audio when reading it with torchaudio. \n",
    "                                              transforms = [#wf.AudioAugment(reverb = 100, snr = 15, pitch = 150, p = [0.5, 0.5, 0.5]),\n",
    "                                              wf.TransformMono(), \n",
    "                                              wf.Padder(22050*10)]) #sr * length of the clip\n",
    "\n",
    "\n",
    "folder = '/work3/s164419/01005WakeWordData/RNN_data/val/'\n",
    "val_dataset = WakewordDataset(f=T.Spectrogram(hop_length=40), folder=folder, normalize=True, #normalize the audio when reading it with torchaudio. \n",
    "                                              transforms = [#wf.AudioAugment(reverb = 100, snr = 15, pitch = 150, p = [0.5, 0.5, 0.5]),\n",
    "                                              wf.TransformMono(), \n",
    "                                              wf.Padder(22050*10)]) #sr * length of the clip\n",
    "\n",
    "\n",
    "#Create the loaders.\n",
    "if device.type == 'cpu':\n",
    "    batch_size = 16\n",
    "else:\n",
    "    batch_size = 512\n",
    "    \n",
    "train_loader  = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "val_loader  = DataLoader(val_dataset, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71239c96-65af-404e-aa1d-113c7e0caeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "                              nn.Conv2d(in_channels = 1, out_channels = 1, kernel_size = (1, 6), stride=(1, 4)),\n",
    "                              nn.BatchNorm2d(num_features = 1),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Dropout(p=0.2),\n",
    "                            )\n",
    "        \n",
    "        self.GRU1 = nn.GRU(batch_first = True, input_size = 201, hidden_size = 128)\n",
    "        self.dropout1 = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.GRU2 = nn.GRU(batch_first = True, input_size = 128, hidden_size = 128)\n",
    "        self.dropout2 = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.fc = nn.Linear(in_features = 128, out_features = 2, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x    = self.conv(x) # nbatch x 1 x 201 x L      -> nbatch x 196 x 201 x L/4ish. \n",
    "        x    = x.squeeze(1) # nbatch x 1 x 201 x L/4ish -> nbatch x 201 x L/4ish.\n",
    "        x    = x.permute(0,2,1) # nbatch x 201 x L/4ish -> nbatch x L/4ish x 201\n",
    "        \n",
    "        x, _ = self.GRU1(x) #nbatch x L/4ish x 201 -> nbatch x L/4ish x 128\n",
    "        x    = self.dropout1(x)\n",
    "        \n",
    "        x, _ = self.GRU2(x)\n",
    "        x    = self.dropout2(x)\n",
    "        \n",
    "        #Output - no softmax!\n",
    "        x    = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5983229",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels = 1, out_channels = 1, kernel_size = (1, 15), stride = (1, 4))\n",
    "        self.GRU  = nn.GRU(batch_first = True, input_size = 201, hidden_size = 128, num_layers = 2, dropout=0.2)\n",
    "        \n",
    "        self.fc1  = nn.Linear(in_features = 128, out_features = 64)\n",
    "        self.fc2  = nn.Linear(in_features = 64,  out_features = 2)\n",
    "\n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = x.squeeze(1)     \n",
    "        x = x.permute(0,2,1) \n",
    "\n",
    "        output, _ = self.GRU(x)\n",
    "\n",
    "        x = output.squeeze(0)\n",
    "\n",
    "        x    = self.fc1(x)\n",
    "        x    = self.ReLU(x)\n",
    "        x    = self.fc2(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b20afd8-5686-4e17-acd6-2ae0cebcf000",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = SimpleRNN().to(device)\n",
    "\n",
    "#weights = torch.tensor([1., 5.]).to(device)\n",
    "#criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer = optim.Adam(rnn.parameters())\n",
    "optimizer = optim.SGD(rnn.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "nepoch = 5\n",
    "\n",
    "epochs, train_losses, test_losses = wf.train_rnn(rnn, criterion, optimizer, train_loader, device, nepoch, val_loader = val_loader, silent=False, scheduler=scheduler)\n",
    "\n",
    "with open('/work3/s164419/01005WakeWordData/models/RNN_V04_results.p', 'wb') as f:\n",
    "    pickle.dump([epochs, train_losses, test_losses], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3293d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(rnn.state_dict(), '/work3/s164419/01005WakeWordData/models/RNN_V04.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3682f276",
   "metadata": {},
   "source": [
    "# Load a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee7cba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_batch = False\n",
    "\n",
    "if predict_batch:\n",
    "\n",
    "    rnn_pretrained = SimpleRNN()\n",
    "    rnn_pretrained.load_state_dict(torch.load('/work3/s164419/01005WakeWordData/models/RNN_V04.pth', map_location=torch.device('cpu')))\n",
    "\n",
    "    with open('/work3/s164419/01005WakeWordData/models/RNN_V04_results.p', 'rb') as f:\n",
    "        epochs, train_losses, test_losses = pickle.load(f)\n",
    "\n",
    "\n",
    "    audio, sr, x, target, path, ID = next(iter(train_loader))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = rnn_pretrained(x)\n",
    "        p = torch.softmax(outputs, dim=-1)\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=4, ncols=4, figsize=(15,10))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for k, ax in enumerate(axs):\n",
    "        ax.plot(p[k,:,1])\n",
    "        ax.plot(target[k])\n",
    "        #ax.set_title(path[k])\n",
    "        \n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_losses)\n",
    "plt.plot(epochs, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eafe29",
   "metadata": {},
   "source": [
    "# Dummy - Train on a Single Utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07398fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights = torch.tensor([1., 5.]).to(device)\n",
    "#criterion = nn.CrossEntropyLoss(weight = weights)\n",
    "\n",
    "#scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73df39c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rnn = SimpleRNN().to(device)\n",
    "#\n",
    "#weights = torch.tensor([1., 5.]).to(device)\n",
    "#criterion = nn.CrossEntropyLoss(weight = weights)\n",
    "#optimizer = optim.Adam(rnn.parameters())\n",
    "#\n",
    "#audio, sr, x, target, path, ID = next(iter(train_loader))\n",
    "#\n",
    "#fig = plt.figure()\n",
    "#ax = plt.gca()\n",
    "#\n",
    "#for i in range(5000):\n",
    "#    ipd.clear_output(wait=True)\n",
    "#\n",
    "#    inputs = x[:2,:,:,:]\n",
    "#    targets = target[:2]\n",
    "#    outputs = rnn(inputs)\n",
    "#\n",
    "#    # zero the parameter gradients\n",
    "#    optimizer.zero_grad()\n",
    "#\n",
    "#    # forward + backward + optimize\n",
    "#    #outputs = rnn(inputs.float())\n",
    "\n",
    "#    #If we use CrossEntropyLoss\n",
    "#    outputs = rnn(inputs.float()) \n",
    "#\n",
    "#    loss = criterion(outputs.permute(0,2,1), targets.long()) #Permute according to https://discuss.pytorch.org/t/loss-functions-for-batches/20488/6\n",
    "#    loss.backward()\n",
    "#    optimizer.step()\n",
    "#\n",
    "#    p = torch.softmax(outputs, dim=-1)\n",
    "#    \n",
    "#    idx = np.random.randint(low=0, high=2)\n",
    "#    plt.plot(p[idx,:, 1].detach())\n",
    "#    plt.plot(target[idx].detach())\n",
    "#    plt.title(i)\n",
    "#    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
