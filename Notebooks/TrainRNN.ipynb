{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47f2369f-8501-439f-b138-eb6e7dead030",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fc0eac-b3ae-44a1-ac08-3f28e3ab910a",
   "metadata": {},
   "source": [
    "Here, the dataset is generated using:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324eb71b-ad67-4edb-acdc-592143ba9b98",
   "metadata": {},
   "source": [
    "`python downloader.py --export-folder 'RNN_data' --splits 'train' --cliplength 10 --balance 1:1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c435272-c064-42a6-80b9-22a5a5c85e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Import torch stuff.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "#pip install git+https://github.com/facebookresearch/WavAugment.git\n",
    "import augment\n",
    "\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "#Append the path outside so we can load bom1.\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import bom1.wakeword as wf\n",
    "import bom1.bom1 as bom1\n",
    "from   bom1.toolbox import WakewordDataset\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import os\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25a15dd7-bba1-4bcc-98f5-8a547b3677f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook is running on the cuda.\n"
     ]
    }
   ],
   "source": [
    "#Set the notebook to run on the GPU, if available.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f'This notebook is running on the {device.type}.')\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.current_device()\n",
    "    torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "950307d9-2521-4b7c-bae4-3be38d28227f",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/work3/s164419/01005WakeWordData/RNN_data/train/'\n",
    "train_dataset = WakewordDataset(f=T.Spectrogram(hop_length=40), folder=folder, normalize=True, #normalize the audio when reading it with torchaudio. \n",
    "                                              transforms = [#wf.AudioAugment(reverb = 100, snr = 15, pitch = 150, p = [0.5, 0.5, 0.5]),\n",
    "                                              wf.TransformMono(), \n",
    "                                              wf.Padder(22050*10)]\n",
    "                                              ) #sr * length of the clip\n",
    "\n",
    "folder = '/work3/s164419/01005WakeWordData/RNN_data/val/'\n",
    "val_dataset = WakewordDataset(f=T.Spectrogram(hop_length=40), folder=folder, normalize=True, #normalize the audio when reading it with torchaudio. \n",
    "                                              transforms = [#wf.AudioAugment(reverb = 100, snr = 15, pitch = 150, p = [0.5, 0.5, 0.5]),\n",
    "                                              wf.TransformMono(), \n",
    "                                              wf.Padder(22050*10)]\n",
    "                                              ) #sr * length of the clip\n",
    "\n",
    "\n",
    "#Create the loaders.\n",
    "if device.type == 'cpu':\n",
    "    batch_size = 16\n",
    "else:\n",
    "    batch_size = 512\n",
    "    \n",
    "train_loader  = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "val_loader    = DataLoader(val_dataset, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5be67b6b-18a9-46ee-bd22-a98ce23142c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bom1.models import RNN_V1\n",
    "\n",
    "rnn = RNN_V1().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight = torch.Tensor([20]).to(device))\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=0.0001)\n",
    "nepoch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10309687-2305-4ab7-b9ca-1d4782e5deb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|                                             | 0/50 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses   = []\n",
    "\n",
    "for epoch in tqdm(range(nepoch), total=nepoch, desc='Epoch'):\n",
    "    \n",
    "    train_loss = 0\n",
    "    val_loss   = 0\n",
    "    \n",
    "    for data in train_loader:\n",
    "        #Reset the gradients.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #Fetch the data\n",
    "        x, contains_catchphrase, path = data\n",
    "\n",
    "        x = x.to(device)\n",
    "\n",
    "        #Forward pass\n",
    "        #outputs = rnn(x)\n",
    "        outputs = rnn(x).squeeze(-1)\n",
    "        \n",
    "        #Construct the targets.\n",
    "        targets = torch.zeros(torch.Size([outputs.shape[0], outputs.shape[1]]))\n",
    "        #825:873 is equivalent from 6s to 6.36s.\n",
    "        targets[torch.where(contains_catchphrase)[0], 618:756] = 1\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        #Calculate the loss\n",
    "        #loss = criterion(outputs.permute(0,2,1), targets.long())\n",
    "        loss = criterion(outputs, targets) #BCE Loss.\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "     \n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            #Fetch the data\n",
    "            x, contains_catchphrase, path = data\n",
    "            x = x.to(device)\n",
    "            \n",
    "            #Forward pass\n",
    "            #outputs = rnn(x)\n",
    "            outputs = rnn(x).squeeze(-1)\n",
    "            \n",
    "            #Construct the targets.\n",
    "            targets = torch.zeros(torch.Size([outputs.shape[0], outputs.shape[1]]))\n",
    "            #825:873 is equivalent from 6s to 6.36s.\n",
    "            #targets[torch.where(contains_catchphrase)[0], 825:873] = 1\n",
    "            \n",
    "            \n",
    "            #618 : 756 is equivalent with the entire duration (1s) of the \"Kan I se det\", placed in the middle of the clip.\n",
    "            targets[torch.where(contains_catchphrase)[0], 618:756] = 1\n",
    "            \n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            #Calculate the validation loss\n",
    "            #loss = criterion(outputs.permute(0,2,1), targets.long())\n",
    "            loss = criterion(outputs, targets) #BCE Loss.\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    #Find the average batch loss.\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    val_loss   = val_loss / len(val_loader)\n",
    "    \n",
    "    #Append the training loss and the validation loss.\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    torch.save(rnn.state_dict(), os.path.join('/work3/s164419/01005WakeWordData/models/RNN_V1', 'model.pth'))\n",
    "    \n",
    "    print(f'[{epoch}] Train loss: {train_loss}, Val loss: {val_loss}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480b6108-93f6-4cfc-8be3-67d0ed9d877b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, 'b--', label='train')\n",
    "plt.plot(val_losses, 'r--', label='val')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43d765f-1821-4241-98cd-7938fcd2b7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetch the data\n",
    "x, contains_catchphrase, path = data\n",
    "\n",
    "x = x.to(device)\n",
    "\n",
    "#Forward pass\n",
    "#outputs = rnn(x)\n",
    "outputs = rnn(x).squeeze(-1)\n",
    "\n",
    "#Construct the targets.\n",
    "targets = torch.zeros(torch.Size([outputs.shape[0], outputs.shape[1]]))\n",
    "#825:873 is equivalent from 6s to 6.36s.\n",
    "targets[torch.where(contains_catchphrase)[0], 618:756] = 1\n",
    "targets = targets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f0e792-8a5c-401b-94c4-153f2f18bfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_positive = np.random.choice(torch.where(contains_catchphrase)[0])\n",
    "idx_negative = np.random.choice(torch.where(contains_catchphrase==0)[0])\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "audio, sr = torchaudio.load(path[idx_positive])\n",
    "plt.plot(audio.mean(axis=0))\n",
    "plt.title('Positive sound')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "audio, sr = torchaudio.load(path[idx_negative])\n",
    "plt.plot(audio.mean(axis=0))\n",
    "plt.title('Negative sound')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.title('Positive example')\n",
    "plt.plot(torch.sigmoid(outputs[idx_positive]).detach().cpu())\n",
    "plt.plot(targets[idx_positive].detach().cpu())\n",
    "plt.axhline(0.5, linestyle='--', color='black')\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.title('Negative example')\n",
    "plt.plot(torch.sigmoid(outputs[idx_negative]).detach().cpu())\n",
    "plt.plot(targets[idx_negative].detach().cpu())\n",
    "plt.axhline(0.5, linestyle='--', color='black')\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
