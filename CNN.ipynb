{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63993ff5-f509-4df7-bb17-aea0a6907f2d",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf1d4b6-039f-40c2-9642-d8938ec423bc",
   "metadata": {},
   "source": [
    "First, we want to import some of the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14484ee6-4642-4851-8aab-c4300e67846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"2,3\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Import torch stuff.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "#pip install git+https://github.com/facebookresearch/WavAugment.git\n",
    "import augment\n",
    "\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "import bom1.wakeword as wf\n",
    "import bom1.bom1 as bom1\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a9e2724-427f-432d-8a67-a7f54c020bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook is running on the cuda.\n"
     ]
    }
   ],
   "source": [
    "#Set the notebook to run on the GPU, if available.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f'This notebook is running on the {device.type}.')\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.current_device()\n",
    "    torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbfece1-f070-4093-81ce-26005731454b",
   "metadata": {},
   "source": [
    "# Defining Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd107bb2-21cb-4228-88f9-861ccc6f8e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WakewordDataset(Dataset):\n",
    "    '''\n",
    "    Construct a dataset with sound files.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, f, folder, sr = 22050, normalize = True, transforms=None):\n",
    "        \n",
    "        #assert not ((folder is None) and (dataframe is None)), 'Both folder and dataframe cannot be none.'\n",
    "        #assert (folder is None) or (dataframe is None), 'Either folder should be None or dataframe should be None.'\n",
    "    \n",
    "        #if folder is not None: \n",
    "            #f is the function that takes audio and returns the spectrogram.\n",
    "        self.paths  = [os.path.join(folder, x) for x in os.listdir(folder)]\n",
    "\n",
    "        folderinfo  = [wf.info_from_path(x) for x in os.listdir(folder)] #Already here, it's shuffled.\n",
    "        self.ID, self.t1, self.t2, self.target = [x[0] for x in folderinfo], [x[1] for x in folderinfo], [x[2] for x in folderinfo], [x[3] for x in folderinfo]\n",
    "            \n",
    "        #    self.folder = True\n",
    "            \n",
    "        #elif dataframe is not None:\n",
    "\n",
    "       #     self.paths = [os.path.join('/work3/s164419/01005WakeWordData/lectures', f'{x}.wav') for x in dataframe['ID'].tolist()]\n",
    "            \n",
    "            #Fetch all of the stuff from the dataframe.\n",
    "       #     self.ID     = dataframe['ID'].tolist()\n",
    "       #     self.t1     = dataframe['t1'].tolist()\n",
    "       #     self.t2     = dataframe['t2'].tolist()\n",
    "       #     self.target = dataframe['class'].tolist()\n",
    "            \n",
    "       #     self.folder = False\n",
    "            \n",
    "        #else:\n",
    "        #    raise NotImplementedError()\n",
    "        \n",
    "        \n",
    "        self.transforms = transforms\n",
    "        self.f          = f\n",
    "        self.normalize  = normalize\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path            = self.paths[idx]\n",
    "        \n",
    "        #if self.folder:\n",
    "        audio, sr, x    = wf.load_data(path, f = self.f, transforms=self.transforms, normalize=self.normalize)\n",
    "        #else:\n",
    "        #    t1, t2 = self.t1[idx], self.t2[idx]\n",
    "        #    audio, sr, x    = wf.load_data(path, f = self.f, transforms=self.transforms, normalize=self.normalize, t1=t1, t2=t2)\n",
    "            \n",
    "        target          = self.target[idx]\n",
    "        ID              = self.ID[idx] \n",
    "        \n",
    "        return audio, sr, x, target, path, ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cefb92ae-da2d-41c8-8c71-73036f854168",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = WakewordDataset(folder='/work3/s164419/01005WakeWordData/every50_1s/train/', f = T.Spectrogram(), \n",
    "                                normalize=True, #normalize the audio when reading it with torchaudio. \n",
    "                                transforms = [#wf.AudioAugment(reverb = 100, snr = 15, pitch = 150, p = [0.5, 0.5, 0.5]),\n",
    "                                              wf.TransformMono(), \n",
    "                                              wf.Padder(22050)] #sr * cliplength\n",
    "                               )\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=128, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e635f5d0-1013-4a5b-a349-472c44206fd5",
   "metadata": {},
   "source": [
    "# CNN with default spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496137f4-2c96-4985-a602-b9d74df6c96b",
   "metadata": {},
   "source": [
    "Let us make a completely vanilla CNN where `x` is the default spectrogram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6f5a86d-eefa-4e7d-8115-7ca74de07094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.57 ms ± 151 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit audio, sr, x, target, path, ID = train_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b6e7b6-1190-428a-8ad1-e5adea0ba385",
   "metadata": {},
   "source": [
    "Let's read in the first batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "671fd287-6f90-4ac2-b9da-c66c691e5c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14 s ± 32.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit audio, sr, x, target, path, ID = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb207e0-9de3-432b-8dc3-f89442f099b7",
   "metadata": {},
   "source": [
    "## Defining the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6033340d-1a8a-4214-884f-11345355aa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sequential = nn.Sequential(\n",
    "                              nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "                              nn.BatchNorm2d(num_features=16),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Dropout2d(p=0.2),\n",
    "\n",
    "                              nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "                              nn.BatchNorm2d(num_features=16),\n",
    "                              nn.ReLU(),\n",
    "                              \n",
    "\n",
    "                              nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "                              nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "                              nn.BatchNorm2d(num_features=32),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Dropout2d(p=0.2),\n",
    "\n",
    "                              nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "                              nn.BatchNorm2d(num_features=32),\n",
    "                              nn.ReLU(),\n",
    "                              \n",
    "\n",
    "                              nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "                              nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "                              nn.BatchNorm2d(num_features=64),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Dropout2d(p=0.2),\n",
    "            \n",
    "                              nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "                              nn.BatchNorm2d(num_features=64),\n",
    "                              nn.ReLU(),\n",
    "\n",
    "                              nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "                              nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "                              nn.BatchNorm2d(num_features=128),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Dropout2d(p=0.2),\n",
    "            \n",
    "                              nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "                              nn.BatchNorm2d(num_features=128),\n",
    "                              nn.ReLU(),\n",
    "\n",
    "                              nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "                              nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size=3, stride=1),\n",
    "                              nn.BatchNorm2d(num_features=256),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Dropout2d(p=0.2),\n",
    "            \n",
    "                              nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size=3, stride=1),\n",
    "                              nn.BatchNorm2d(num_features=256),\n",
    "                              nn.ReLU(),\n",
    "\n",
    "                              #nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "                              #nn.Conv2d(in_channels = 256, out_channels = 512, kernel_size=3, stride=1),\n",
    "                              #nn.BatchNorm2d(num_features=512),\n",
    "                              #nn.ReLU(),\n",
    "\n",
    "                              #nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size=3, stride=1),\n",
    "                              #nn.BatchNorm2d(num_features=512),\n",
    "                              #nn.ReLU(),\n",
    "\n",
    "                              #Fully connected part\n",
    "                              nn.Conv2d(in_channels=256, out_channels=256*8*2, kernel_size=(8,2)),\n",
    "                              nn.ReLU(),\n",
    "\n",
    "                              #nn.Conv2d(in_channels=512*2*2, out_channels=512*2*2, kernel_size=1),\n",
    "                              #nn.ReLU(),\n",
    "\n",
    "                              #Output - no softmax!\n",
    "                              nn.Conv2d(in_channels=256*8*2, out_channels=2, kernel_size=1),\n",
    "                            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.sequential(x)\n",
    "        x = torch.squeeze(x, 3)\n",
    "        x = torch.squeeze(x, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a45e9d-b8ad-4ed1-a4eb-f6b1ca5a7bc0",
   "metadata": {},
   "source": [
    "## Constructing the dataloaders, network and some unit testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c362e982-5def-46e7-ac35-214e548d9c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the datasets.\n",
    "train_dataset = WakewordDataset(folder='/work3/s164419/01005WakeWordData/every50_1s/train/', f = T.Spectrogram(), \n",
    "                                normalize=True, #normalize the audio when reading it with torchaudio. \n",
    "                                transforms = [#wf.AudioAugment(reverb = 100, snr = 15, pitch = 150, p = [0.5, 0.5, 0.5]),\n",
    "                                              wf.TransformMono(), \n",
    "                                              wf.Padder(22050)]\n",
    "                               )\n",
    "\n",
    "val_dataset = WakewordDataset(folder='/work3/s164419/01005WakeWordData/every50_1s/val/', f = T.Spectrogram(), \n",
    "                                normalize=True, #normalize the audio when reading it with torchaudio. \n",
    "                                transforms = [#wf.AudioAugment(reverb = 100, snr = 15, pitch = 150, p = [0.5, 0.5, 0.5]),\n",
    "                                              wf.TransformMono(), \n",
    "                                              wf.Padder(22050)]\n",
    "                               )\n",
    "\n",
    "#test_dataset = WakewordDataset(dataframe='/work3/s164419/01005WakeWordData/every50_1s/test/', f = T.Spectrogram(), \n",
    "#                                normalize=True, #normalize the audio when reading it with torchaudio. \n",
    "#                                transforms = [#wf.AudioAugment(reverb = 100, snr = 15, pitch = 150, p = [0.5, 0.5, 0.5]),\n",
    "#                                              wf.TransformMono(), \n",
    "#                                              wf.Padder(22050)]\n",
    "#                               )\n",
    "\n",
    "#Create the loaders.\n",
    "batch_size = 256\n",
    "train_loader  = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "val_loader    = DataLoader(val_dataset  , shuffle=True, batch_size=batch_size)\n",
    "#test_loader   = DataLoader(test_dataset , shuffle=True, batch_size=batch_size)\n",
    "\n",
    "#Let us load a batch for unit tests.\n",
    "audio, sr, x, targets, paths, ids = next(iter(train_loader))\n",
    "\n",
    "#Construct the network.\n",
    "cnn = CNN()\n",
    "\n",
    "assert cnn(x).shape == torch.Size([batch_size, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5337930f-d665-49f6-bc0f-060163f6573b",
   "metadata": {},
   "source": [
    "## Training the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a484cc83-93d4-4213-baec-5dd9061d9953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6096e03d9434235b60435871c6ed67a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30ba751ffa114bfbb613c769032c744a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/5634 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17783/339692459.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#optimizer = optim.SGD(cnn.parameters(), lr=0.01, momentum = 0.9)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/01005WakeWord/bom1/wakeword/train_cnn.py\u001b[0m in \u001b[0;36mtrain_cnn\u001b[0;34m(model, criterion, optimizer, train_loader, val_loader, device, nepoch, silent)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mminibatch_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;31m# get the inputs; data is a list of [inputs, labels]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/01005WakeWord/venv/lib/python3.8/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/01005WakeWord/venv/lib/python3.8/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/01005WakeWord/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/01005WakeWord/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/01005WakeWord/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/01005WakeWord/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_17783/3243318852.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m#if self.folder:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mwf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;31m#else:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m#    t1, t2 = self.t1[idx], self.t2[idx]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/01005WakeWord/bom1/wakeword/load_data.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(path, f, sr, normalize, transforms)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/01005WakeWord/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/01005WakeWord/venv/lib/python3.8/site-packages/torchaudio/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, waveform)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mFourier\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mwindow\u001b[0m \u001b[0mhops\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \"\"\"\n\u001b[0;32m--> 110\u001b[0;31m         return F.spectrogram(\n\u001b[0m\u001b[1;32m    111\u001b[0m             \u001b[0mwaveform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/01005WakeWord/venv/lib/python3.8/site-packages/torchaudio/functional/functional.py\u001b[0m in \u001b[0;36mspectrogram\u001b[0;34m(waveform, pad, window, n_fft, hop_length, win_length, power, normalized, center, pad_mode, onesided, return_complex)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;31m# default values are consistent with librosa.core.spectrum._spectrogram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     spec_f = torch.stft(\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwaveform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mn_fft\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_fft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/01005WakeWord/venv/lib/python3.8/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mstft\u001b[0;34m(input, n_fft, hop_length, win_length, window, center, pad_mode, normalized, onesided, return_complex)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextended_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msignal_dim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m     return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n\u001b[0m\u001b[1;32m    472\u001b[0m                     normalized, onesided, return_complex)\n\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cnn = CNN()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=1e-3)\n",
    "#optimizer = optim.SGD(cnn.parameters(), lr=0.01, momentum = 0.9)\n",
    "\n",
    "epochs, train_losses, train_accs, val_losses, val_accs = wf.train_cnn(cnn, criterion, optimizer, train_loader, val_loader, device, nepoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dc9cb7-b0c2-42d8-a760-5fde86e9e871",
   "metadata": {},
   "source": [
    "# Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac951de-a252-4438-8417-e079948632f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cnn.state_dict(), '/work3/s164419/01005WakeWordData/models/cnn_1_to_3_1s.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce2d67a-0bcc-4c4e-93e7-3a929d1b8368",
   "metadata": {},
   "source": [
    "# Evaluate the Model on a Lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06094610-ed2f-4069-b42b-d23db3f71b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN()\n",
    "cnn.load_state_dict(torch.load('/work3/s164419/01005WakeWordData/models/cnn_1_to_3_1s.pth'))\n",
    "cnn.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c404ae-c9f9-4f3f-a52d-c7f371aa728f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, test = wf.get_splits()\n",
    "\n",
    "ID = test['ID'].unique()[25]\n",
    "\n",
    "with torch.no_grad():\n",
    "    data = wf.load_data(f'/work3/s164419/01005WakeWordData/lectures/{ID}.wav', T.Spectrogram(), normalize=True, transforms=[wf.TransformMono()])\n",
    "    audio, sr, x = data\n",
    "    outputs = cnn(x.unsqueeze(0).to(device))\n",
    "    p = outputs.squeeze(0).softmax(dim=0).detach().cpu().numpy().T[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24371443-7229-4313-b311-6376d4f1a70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')\n",
    "t = np.linspace(0, wf.lecture_durations()[ID], len(p))\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(t, p, alpha=0.5, label='Classified')\n",
    "plt.vlines(test.loc[test['ID'] == ID][['t1', 't2']].mean(axis=1).to_numpy(), ymin=0, ymax=1, color='r', linestyles='--', label='GT')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70e05a4-5220-4a09-a625-ae084b30b969",
   "metadata": {},
   "outputs": [],
   "source": [
    "[bom1.seconds_to_timestamp(float(x)) for x in t[p > 0.9]][-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa0fcf5-4c7c-4ff0-94c4-3d5a54d1fe01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
